{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9K302jR6r3yU",
      "metadata": {
        "id": "9K302jR6r3yU"
      },
      "outputs": [],
      "source": [
        "# Copyright 2024 Google LLC\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "93c6fd6c",
      "metadata": {},
      "source": [
        "# Curating Effective RAG Evaluation Datasets\n",
        "\n",
        "<table align=\"left\">\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai/build_your_own_grounded_rag_with_vertexai_apis_for_rag.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Run in Colab\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fapplied-ai-engineering-samples%2Fblob%2Fmain%2Fgenai-on-vertex-ai%2Fretrieval_augmented_generation%2Fdiy_rag_with_vertexai%2Fbuild_your_own_grounded_rag_with_vertexai_apis_for_rag.ipynb\">\n",
        "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Run in Colab Enterprise\n",
        "    </a>\n",
        "  </td>    \n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://github.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai/build_your_own_grounded_rag_with_vertexai_apis_for_rag.ipynb\">\n",
        "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
        "    </a>\n",
        "  </td>\n",
        "  <td style=\"text-align: center\">\n",
        "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/applied-ai-engineering-samples/blob/main/genai-on-vertex-ai/retrieval_augmented_generation/diy_rag_with_vertexai/build_your_own_grounded_rag_with_vertexai_apis_for_rag.ipynb\">\n",
        "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
        "    </a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cfc1dcf",
      "metadata": {},
      "source": [
        "| | |\n",
        "|-|-|\n",
        "| Author(s) | [Ken Lee](https://github.com/kenleejr) |\n",
        "| Reviewers(s) | [Abhishek Bhagwat](https://github.com/Abhishekbhagwat)|\n",
        "| Last updated | 2024-10-07 |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HFXYv9sOVkYq",
      "metadata": {
        "id": "HFXYv9sOVkYq"
      },
      "source": [
        "Deploying a chatbot or retrieval augmented generation (RAG) application to real users provides a wealth of valuable data.  User queries reveal insights into their needs, the products they engage with, and the effectiveness of the chatbot itself. This data is crucial for both understanding your users and continuously evaluating the performance of your deployed system.\n",
        "\n",
        "![image_link](./cluster_diagram.png)\n",
        "\n",
        "\n",
        "This notebook demonstrates how to leverage Gemini to accelerate the analysis and summarization of real user queries from a production RAG system or chatbot. By analyzing these queries, we can identify a representative set of questions to form an evaluation dataset, establishing a foundation for continuous evaluation.\n",
        "\n",
        "\n",
        "This process aims to answer the following questions:\n",
        "\n",
        "- What general categories of questions are users asking? What problems are they encountering?\n",
        "\n",
        "- What topics are prevalent in user conversations?\n",
        "\n",
        "- What sentiments are users expressing?\n",
        "\n",
        "Inspired by a [Weights and Biases article](https://wandb.ai/wandbot/wandbot-eval/reports/How-to-Evaluate-an-LLM-Part-1-Building-an-Evaluation-Dataset-for-our-LLM-System--Vmlldzo1NTAwNTcy), this notebook extends those concepts by utilizing Gemini's capabilities.  Gemini's large context window allows for rapid exploratory data analysis (EDA) of clustered questions, even with extensive datasets, facilitating efficient metadata extraction and informed selection of an evaluation dataset.  This, in turn, enables the construction of a robust and representative evaluation set for the RAG system."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0557777",
      "metadata": {},
      "source": [
        "# üé¨ Getting Started\n",
        "\n",
        "The following steps are necessary to run this notebook, no matter what notebook environment you're using.\n",
        "\n",
        "If you're entirely new to Google Cloud, [get started here](https://cloud.google.com/docs/get-started).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3eabeeb",
      "metadata": {},
      "source": [
        "### Google Cloud Permissions\n",
        "\n",
        "**To run the complete Notebook, including the optional section, you will need to have the [Owner role](https://cloud.google.com/iam/docs/understanding-roles) for your project.**\n",
        "\n",
        "If you want to skip the optional section, you need at least the following [roles](https://cloud.google.com/iam/docs/granting-changing-revoking-access):\n",
        "* **`roles/serviceusage.serviceUsageAdmin`** to enable APIs\n",
        "* **`roles/iam.serviceAccountAdmin`** to modify service agent permissions\n",
        "* **`roles/aiplatform.user`** to use AI Platform components"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9c9284b",
      "metadata": {},
      "source": [
        "### Install Vertex AI SDK and Other Required Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TMrCwE6KVsW3",
      "metadata": {
        "collapsed": true,
        "id": "TMrCwE6KVsW3"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq llama-index \\\n",
        "llama-index-llms-vertex \\\n",
        "llama-index-embeddings-vertex \\\n",
        "python-louvain \\\n",
        "tiktoken \\\n",
        "aiofiles \\\n",
        "annotated-types \\\n",
        "python-fasthtml"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5447b19b",
      "metadata": {},
      "source": [
        "### Restart Runtime\n",
        "\n",
        "To use the newly installed packages in this Jupyter runtime, you must restart the runtime. You can do this by running the cell below, which will restart the current kernel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5e675c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Restart kernel after installs so that your environment can access the new packages\n",
        "import IPython\n",
        "\n",
        "app = IPython.Application.instance()\n",
        "app.kernel.do_shutdown(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51fc3b0f",
      "metadata": {},
      "source": [
        "<div class=\"alert alert-block alert-warning\">\n",
        "<b>‚ö†Ô∏è The kernel is going to restart. Please wait until it is finished before continuing to the next step. ‚ö†Ô∏è</b>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f67891c",
      "metadata": {},
      "source": [
        "### Authenticate\n",
        "\n",
        "If you're using Colab, run the code in the next cell. Follow the popups and authenticate with an account that has access to your Google Cloud [project](https://cloud.google.com/resource-manager/docs/creating-managing-projects#identifying_projects).\n",
        "\n",
        "If you're running this notebook somewhere besides Colab, make sure your environment has the right Google Cloud access. If that's a new concept to you, consider looking into [Application Default Credentials for your local environment](https://cloud.google.com/docs/authentication/provide-credentials-adc#local-dev) and [initializing the Google Cloud CLI](https://cloud.google.com/docs/authentication/gcloud). In many cases, running `gcloud auth application-default login` in a shell on the machine running the notebook kernel is sufficient.\n",
        "\n",
        "More authentication options are discussed [here](https://cloud.google.com/docs/authentication)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "039a18d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab authentication.\n",
        "import sys\n",
        "\n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "    print('Authenticated')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DGTF2qUYVp0x",
      "metadata": {
        "id": "DGTF2qUYVp0x"
      },
      "source": [
        "### Set Google Cloud project information and Initialize Vertex AI SDK\n",
        "\n",
        "To get started using Vertex AI, you must have an existing Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com).\n",
        "\n",
        "Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment).\n",
        "\n",
        "Make sure to change `PROJECT_ID` in the next cell. You can leave the values for `REGION` unless you have a specific reason to change them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ODApYPsB6nNR",
      "metadata": {
        "id": "ODApYPsB6nNR"
      },
      "outputs": [],
      "source": [
        "import vertexai\n",
        "\n",
        "PROJECT_ID = \"<enter-your-project-id>\"\n",
        "REGION = \"us-central1\"\n",
        "\n",
        "TEST_RUN = False\n",
        "CLUSTERING_NEIGHBORHOOD_SIZE = 5\n",
        "\n",
        "vertexai.init(\n",
        "    project=PROJECT_ID,\n",
        "    location=REGION\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7GLS_O8p1hSW",
      "metadata": {
        "id": "7GLS_O8p1hSW"
      },
      "source": [
        "## Prepare the dataset\n",
        "\n",
        "For this demo we are using a hypothetical dataset of questions about Google Cloud Services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "I9Ll-ZSrQtcI",
      "metadata": {
        "id": "I9Ll-ZSrQtcI"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "if TEST_RUN:\n",
        "  df = pd.DataFrame({\"Prompt\": [\"What is RAG?\", \"What is life?\", \"What is football?\", \"Who am I?\"],\n",
        "                   \"answer\": [\"Retrieval Augmented Generation\", \"Love\", \"National Football League\", \"Human\"]})\n",
        "else:\n",
        "  df = pd.read_csv(CSV_PATH)\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Q9nFwjmP72Ka",
      "metadata": {
        "id": "Q9nFwjmP72Ka"
      },
      "source": [
        "### Dataset Preprocessing\n",
        "\n",
        "Real world RAG systems have some anomalies in terms of the search queries - often, you will encounter single word queries or typos. In this step, we will preprocess and clean the dataset to remove the following types of queries:\n",
        "- Very short and very long queries\n",
        "- Near duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "l4H8oYXq6wOK",
      "metadata": {
        "id": "l4H8oYXq6wOK"
      },
      "outputs": [],
      "source": [
        "# add question_len column to get length of questions in dataset\n",
        "df[\"question_len\"] = df[\"Question\"].apply(lambda x: len(x))\n",
        "\n",
        "# filter and discard questions with too little or too many characters\n",
        "df = df[(df.question_len > 5) & (df.question_len < 1000)]\n",
        "\n",
        "# view cleaned dataset\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c5d108",
      "metadata": {},
      "source": [
        "#### Visualize distribution of question lengths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YZP8uPVh4K97",
      "metadata": {
        "id": "YZP8uPVh4K97"
      },
      "outputs": [],
      "source": [
        "df.question_len.hist(bins=25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lIB2TGLKRiWu",
      "metadata": {
        "id": "lIB2TGLKRiWu"
      },
      "source": [
        "## Generating the embeddings for the questions\n",
        "\n",
        "Vertex AI embeddings models can generate optimized embeddings for various task types, such as document retrieval, question and answering, and fact verification. Task types are labels that optimize the embeddings that the model generates based on your intended use case.\n",
        "\n",
        "In this example, we will set the `TASK_TYPE` as `RETRIEVAL_DOCUMENT` as this is used to generate embeddings that are optimized for information retrieval\n",
        "\n",
        "Read more about the various `TASK_TYPE` offered by Vertex AI Embedding models [here](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/task-types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VSOR8rf3TZDq",
      "metadata": {
        "id": "VSOR8rf3TZDq"
      },
      "outputs": [],
      "source": [
        "import asyncio\n",
        "from tqdm.asyncio import tqdm_asyncio\n",
        "from typing import List, Optional,  Tuple\n",
        "from vertexai.language_models import TextEmbeddingInput, TextEmbeddingModel\n",
        "from google.cloud import storage\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "\n",
        "async def embed_text_async(\n",
        "    model: TextEmbeddingModel,\n",
        "    texts: List[str] = [\"banana muffins? \", \"banana bread? banana muffins?\"],\n",
        "    task: str = \"RETRIEVAL_DOCUMENT\",\n",
        "    dimensionality: Optional[int] = 768,):\n",
        "    inputs = [TextEmbeddingInput(text, task) for text in texts]\n",
        "    kwargs = dict(output_dimensionality=dimensionality) if dimensionality else {}\n",
        "    embeddings = await model.get_embeddings_async(texts, **kwargs)\n",
        "    return [embedding.values for embedding in embeddings]\n",
        "\n",
        "# embedding model to use\n",
        "model_name = \"text-embedding-004\"\n",
        "embedding_model = TextEmbeddingModel.from_pretrained(model_name)\n",
        "\n",
        "# embed questions from the dataset asynchronously\n",
        "embedded_qs = await tqdm_asyncio.gather(*[embed_text_async(embedding_model,\n",
        "                                        [x[\"Question\"]]) for i, x in df.iterrows()])\n",
        "\n",
        "# collect all async results and flatten into a single list of question embeddings\n",
        "embedded_qs_flattened = [q[0] for q in embedded_qs]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kgTbuokcRl1L",
      "metadata": {
        "id": "kgTbuokcRl1L"
      },
      "source": [
        "## Cluster the Questions\n",
        "\n",
        "While various clustering algorithms can be applied, Louvain community detection is a particularly suitable choice for this task due to its speed and effectiveness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NAlsWHDXz5v9",
      "metadata": {
        "id": "NAlsWHDXz5v9"
      },
      "source": [
        "### Vector-based Retrieval Clustering\n",
        "1. Store your embedded question set in a vector index\n",
        "2. Query the vector index with each question in the dataset, retrieving a topk-sized neighborhood of questions around the query question.\n",
        "3. Form a graph of questions by adding an edge between the query question and each of the retrieved questions\n",
        "4. Perform Louvain or Leiden community detection on the graph to create clusters of questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ML6i053tW3vQ",
      "metadata": {
        "collapsed": true,
        "id": "ML6i053tW3vQ"
      },
      "outputs": [],
      "source": [
        "from llama_index.core import (\n",
        "    VectorStoreIndex,\n",
        "    Settings,\n",
        "    SimpleDirectoryReader,\n",
        "    load_index_from_storage,\n",
        "    StorageContext,\n",
        "    Document\n",
        ")\n",
        "from llama_index.llms.vertex import Vertex\n",
        "from llama_index.embeddings.vertex import VertexTextEmbedding\n",
        "from vertexai.generative_models import HarmCategory, HarmBlockThreshold\n",
        "import networkx as nx\n",
        "from community import community_louvain # pip install python-louvain\n",
        "import google.auth\n",
        "import google.auth.transport.requests\n",
        "\n",
        "credentials = google.auth.default()[0]\n",
        "request = google.auth.transport.requests.Request()\n",
        "credentials.refresh(request)\n",
        "\n",
        "\n",
        "query_list = df[\"Question\"].tolist()\n",
        "query_docs = [Document(text=t) for t in query_list] # To make it LlamaIndex compatible\n",
        "embed_model = VertexTextEmbedding(credentials=credentials, model_name=\"text-embedding-004\")\n",
        "llm = Vertex(model=\"gemini-1.5-pro\",\n",
        "             temperature=0.2,\n",
        "             max_tokens=8192,\n",
        "             safety_settings={\n",
        "                    HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "                    HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "        }\n",
        ")\n",
        "Settings.llm = llm\n",
        "Settings.embed_model = embed_model\n",
        "\n",
        "\n",
        "# Form a local vector index with all our questions\n",
        "vector_index = VectorStoreIndex.from_documents(query_docs)\n",
        "vector_retriever = vector_index.as_retriever(similarity_top_k=CLUSTERING_NEIGHBORHOOD_SIZE)\n",
        "\n",
        "\n",
        "# Create a similarity graph\n",
        "G = nx.Graph()\n",
        "\n",
        "# Get a neighborhood of similar questions by querying the vector index\n",
        "similar_texts = await tqdm_asyncio.gather(*[vector_retriever.aretrieve(text) for i, text in enumerate(query_list)])\n",
        "\n",
        "for i, text in enumerate(query_list):\n",
        "  for s in similar_texts[i]:\n",
        "    G.add_edge(text, s.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "UKhkdk7UcV88",
      "metadata": {
        "collapsed": true,
        "id": "UKhkdk7UcV88"
      },
      "outputs": [],
      "source": [
        "# Apply Louvain Community Detection\n",
        "partition = community_louvain.best_partition(G)\n",
        "df[\"cluster_idx\"] = df[\"Question\"].map(partition)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "ryVyUMhCcb-o",
      "metadata": {
        "id": "ryVyUMhCcb-o"
      },
      "outputs": [],
      "source": [
        "grouped_df = pd.DataFrame(df.groupby(\"cluster_idx\")['Question'].apply(list)).reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kyapkNuvKRZZ",
      "metadata": {
        "collapsed": true,
        "id": "kyapkNuvKRZZ"
      },
      "outputs": [],
      "source": [
        "grouped_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K5UnIAP7KEX5",
      "metadata": {
        "id": "K5UnIAP7KEX5"
      },
      "outputs": [],
      "source": [
        "grouped_df[\"num_questions\"] = grouped_df[\"Question\"].apply(len)\n",
        "grouped_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RzT0fZC-RsaB",
      "metadata": {
        "id": "RzT0fZC-RsaB"
      },
      "source": [
        "## Analyze Clusters Using Gemini\n",
        "\n",
        "We can use Gemini to extract summaries, topics, relevant questions, sentiment...whatever we want from each cluster.\n",
        "\n",
        "This allows us to quickly identify higher level patterns about what people are asking, what problems they are facing, their frustrations, etc.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "8Usbh9JmRvpB",
      "metadata": {
        "id": "8Usbh9JmRvpB"
      },
      "outputs": [],
      "source": [
        "from vertexai.generative_models import GenerativeModel, GenerationConfig\n",
        "from vertexai.generative_models import HarmCategory, HarmBlockThreshold\n",
        "from llama_index.core.program import LLMTextCompletionProgram\n",
        "from llama_index.core.output_parsers import PydanticOutputParser\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import Annotated\n",
        "from enum import Enum\n",
        "from annotated_types import Len\n",
        "\n",
        "num_clusters = grouped_df.shape[0]\n",
        "\n",
        "class Sentiment(Enum):\n",
        "  POSITIVE = \"positive\"\n",
        "  NEGATIVE = \"negative\"\n",
        "  NEUTRAL = \"neutral\"\n",
        "\n",
        "class ClusterSummary(BaseModel):\n",
        "  '''A cluster summary, list of topics, most representative questions, and sentiment associated with a cluster of questions from chat sessions.'''\n",
        "  summary_desc: str\n",
        "  topics: List[str]\n",
        "  most_representative_qs: Annotated[List[str], Len(3, 8)]\n",
        "  sentiment: Sentiment\n",
        "\n",
        "\n",
        "boring_prompt = \"\"\"Please provide a brief summary which captures the nature of the given cluster of questions below in the form of \"Questions concerning ____\".\n",
        "                  \\n Cluster questions:\n",
        "                  \\n {questions_list}\n",
        "                  \\n The clusters titles should not be generic such as \"Google Cloud AI\" or \"Gemini\".\n",
        "                  \\n They need to be specific in order to distinguish the clusters from others which may be similar.\n",
        "                  \\n Also include a list of topic phrases which the questions address, the most representative questions of the cluster, and an overall sentiment. Be sure to follow a consistent format.\"\"\"\n",
        "\n",
        "movie_prompt = \"\"\"You are an expert movie producer for famous movies.\n",
        "                  \\n Please provide a quipy, movie title which captures the essence of the given cluster of questions below.\n",
        "                  \\n Example:\n",
        "                  \\n How does RAG work on Vertex?\n",
        "                  \\n Where can I find documentation on Vertex AI Generative model API?\n",
        "                  \\n What are the pitfals of Gemini vs. Gemma?\n",
        "                  \\n Answer:\n",
        "                  \\n movie title: \"Into the Vertex\"\n",
        "                  \\n representative qs: How does RAG work on Vertex?\n",
        "                  \\n topics: Vertex AI, Vertex AI Generative Model\n",
        "                  \\n sentiment: neutral\n",
        "                  \\n Cluster questions:\n",
        "                  \\n {questions_list}\n",
        "                  \\n Also include a list of topic phrases which the questions address, the most representative questions of the cluster, and an overall sentiment. Be sure to follow a consistent format. \"\"\"\n",
        "\n",
        "async def summarize_cluster(questions: List[str]):\n",
        "  questions_list = \"\\n\".join(questions)\n",
        "  llm_program = LLMTextCompletionProgram.from_defaults(\n",
        "        output_parser=PydanticOutputParser(ClusterSummary),\n",
        "        prompt_template_str=boring_prompt,\n",
        "        verbose=True,\n",
        "    )\n",
        "  try:\n",
        "    cluster_summary = await llm_program.acall(questions_list=questions_list)\n",
        "  except Exception as e:\n",
        "    print(e)\n",
        "    return None\n",
        "  return cluster_summary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9EUj5xwpgMzU",
      "metadata": {
        "id": "9EUj5xwpgMzU"
      },
      "outputs": [],
      "source": [
        "# Summarize each cluster individually\n",
        "cluster_summaries = await tqdm_asyncio.gather(*[summarize_cluster(q[\"Question\"]) for idx, q in grouped_df.iterrows()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3l0OnOUO9pXh",
      "metadata": {
        "collapsed": true,
        "id": "3l0OnOUO9pXh"
      },
      "outputs": [],
      "source": [
        "cluster_summaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "k0U7a6nqZeMP",
      "metadata": {
        "id": "k0U7a6nqZeMP"
      },
      "outputs": [],
      "source": [
        "just_summaries = [c.summary_desc if c else None for c in cluster_summaries]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "oLp3ceBVSEo2",
      "metadata": {
        "id": "oLp3ceBVSEo2"
      },
      "outputs": [],
      "source": [
        "df_grouped_by_cluster = df.groupby(\"cluster_idx\").agg(\"count\")\n",
        "df_grouped_by_cluster[\"cluster_summary\"] = cluster_summaries\n",
        "df_grouped_by_cluster[\"just_summary\"] = just_summaries\n",
        "df_grouped_by_cluster[\"questions_list\"] = grouped_df[\"Question\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "tt3kS_tPA6XL",
      "metadata": {
        "id": "tt3kS_tPA6XL"
      },
      "outputs": [],
      "source": [
        "from fasthtml.common import *\n",
        "from fasthtml.fastapp import *\n",
        "from random import sample\n",
        "from fasthtml.components import Zero_md\n",
        "\n",
        "tlink = Script(src=\"https://cdn.tailwindcss.com\")\n",
        "dlink = Link(rel=\"stylesheet\", href=\"https://cdn.jsdelivr.net/npm/daisyui@4.11.1/dist/full.min.css\")\n",
        "app = FastHTML(hdrs=(dlink, tlink))\n",
        "\n",
        "def Markdown(md, css = ''):\n",
        "    css_template = Template(Style(css), data_append=True)\n",
        "    return Zero_md(css_template, Script(md, type=\"text/markdown\"))\n",
        "\n",
        "def MarkdownWOutBackground(md: str):\n",
        "    css = '.markdown-body {background-color: unset !important; color: unset !important;} .markdown-body table {color: black !important;}'\n",
        "    markdown_wout_background = partial(Markdown, css=css)\n",
        "    return markdown_wout_background(md)\n",
        "\n",
        "def stat_card(num_questions: int):\n",
        "  return Div(\n",
        "    Div('Total Questions', cls='stat-title'),\n",
        "    Div(f'{num_questions}', cls='stat-value'),\n",
        "    cls='stat'\n",
        "  )\n",
        "\n",
        "def cluster_card(cluster_summary: ClusterSummary, questions_list: List[str]):\n",
        "  if cluster_summary.sentiment == Sentiment.NEGATIVE:\n",
        "    badge_color = \"error\"\n",
        "  elif cluster_summary.sentiment == Sentiment.NEUTRAL:\n",
        "    badge_color = \"neutral\"\n",
        "  else:\n",
        "    badge_color = \"success\"\n",
        "  return Div(\n",
        "              Div(\n",
        "                  H2(cluster_summary.summary_desc, cls='card-title'),\n",
        "                  Div(\n",
        "                      stat_card(len(questions_list)),\n",
        "                      Div(cluster_summary.sentiment, cls=f'badge badge-{badge_color}'),\n",
        "                      cls=\"flex flex-row items-center\"\n",
        "                  ),\n",
        "                  H4(\"Representative Questions:\", cls=\"font-bold\"),\n",
        "                  Ul(\n",
        "                      *[Li(q) for q in cluster_summary.most_representative_qs],\n",
        "                      cls='list-disc list-inside mt-2'\n",
        "                  ),\n",
        "                  H4(\"Topics Discussed:\", cls=\"font-bold\"),\n",
        "                  Ul(\n",
        "                      *[Li(t) for t in cluster_summary.topics],\n",
        "                      cls='list-disc list-inside mt-2'\n",
        "                  ),\n",
        "                  cls='card-body'\n",
        "              ),\n",
        "              cls='card bg-base-100 shadow-xl'\n",
        "          )\n",
        "\n",
        "@app.get(\"/\")\n",
        "def cluster_analysis():\n",
        "    return Div(\n",
        "              *[cluster_card(c, q) for c, q in zip(cluster_summaries, df_grouped_by_cluster[\"questions_list\"])],\n",
        "              cls=\"grid grid-cols-2 gap-2\"\n",
        "            )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DjLQVD7MRVn1",
      "metadata": {
        "id": "DjLQVD7MRVn1"
      },
      "source": [
        "## Gemini-generated Cluster Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a4ZayTxHe8V",
      "metadata": {
        "id": "1a4ZayTxHe8V"
      },
      "outputs": [],
      "source": [
        "from starlette.testclient import TestClient\n",
        "client = TestClient(app)\n",
        "r = client.get(\"/\")\n",
        "show(r.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_azGz91Do81r",
      "metadata": {
        "id": "_azGz91Do81r"
      },
      "source": [
        "## Sample Questions from Each Cluster to create the Eval Dataset\n",
        "- We can sample randomly proportional to each cluster's size\n",
        "- Or we can take samples from the most representative questions Gemini identified\n",
        "\n",
        "Probably need to sit down with an SME and compare both:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "vPMNsdMuBWq9",
      "metadata": {
        "collapsed": true,
        "id": "vPMNsdMuBWq9"
      },
      "outputs": [],
      "source": [
        "# Calculate the total number of questions\n",
        "total_questions = df_grouped_by_cluster['question_len'].sum()\n",
        "\n",
        "# Calculate the fraction of questions for each row\n",
        "df_grouped_by_cluster['cluster_fraction'] = df_grouped_by_cluster['question_len'] / total_questions\n",
        "\n",
        "# Function to sample from a list based on the fraction\n",
        "def sample_questions(row, num_samples):\n",
        "    return np.random.choice(row['questions_list'],\n",
        "                            size=int(num_samples * row['cluster_fraction']),\n",
        "                            replace=False).tolist()\n",
        "\n",
        "# Specify the total number of samples you want\n",
        "total_samples = 50\n",
        "\n",
        "# Apply the sampling function to each row\n",
        "df_grouped_by_cluster['proportional_sampled_questions'] = df_grouped_by_cluster.apply(lambda row: sample_questions(row, total_samples), axis=1)\n",
        "\n",
        "# Unroll the DataFrame\n",
        "df_grouped_by_cluster = df_grouped_by_cluster.reset_index()\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "unrolled_proportional_df = df_grouped_by_cluster.apply(lambda x: pd.Series({\n",
        "    'cluster_title': [x[\"just_summary\"]] * len(x['proportional_sampled_questions']),\n",
        "    'sampled_question': x['proportional_sampled_questions']\n",
        "}), axis=1)\n",
        "\n",
        "# Concatenate the series and reset the index\n",
        "unrolled_proportional_df = pd.concat([unrolled_proportional_df['cluster_title'].explode(),\n",
        "                         unrolled_proportional_df['sampled_question'].explode()],\n",
        "                        axis=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nI5MjuDiHZLa",
      "metadata": {
        "collapsed": true,
        "id": "nI5MjuDiHZLa"
      },
      "outputs": [],
      "source": [
        "unrolled_proportional_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "GhK3POqVH1mX",
      "metadata": {
        "collapsed": true,
        "id": "GhK3POqVH1mX"
      },
      "outputs": [],
      "source": [
        "df_grouped_by_cluster[\"gemini_representative_questions_len\"] = df_grouped_by_cluster[\"cluster_summary\"].apply(lambda x: len(x.most_representative_qs))\n",
        "df_grouped_by_cluster[\"gemini_representative_questions\"] = df_grouped_by_cluster[\"cluster_summary\"].apply(lambda x: x.most_representative_qs)\n",
        "# Print the resulting DataFrame\n",
        "unrolled_gemini_df = df_grouped_by_cluster.apply(lambda x: pd.Series({\n",
        "    'cluster_title': [x[\"just_summary\"]] * len(x['gemini_representative_questions']),\n",
        "    'representative_question': x['gemini_representative_questions']\n",
        "}), axis=1)\n",
        "\n",
        "# Concatenate the series and reset the index\n",
        "unrolled_gemini_df = pd.concat([unrolled_gemini_df['cluster_title'].explode(),\n",
        "                         unrolled_gemini_df['representative_question'].explode()],\n",
        "                        axis=1).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e2Wxw8XJtWt",
      "metadata": {
        "id": "8e2Wxw8XJtWt"
      },
      "outputs": [],
      "source": [
        "unrolled_gemini_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90LThbX4ov-5",
      "metadata": {
        "id": "90LThbX4ov-5"
      },
      "source": [
        "### Save Results to CSV\n",
        "- We do need to obtain ground truth answers\n",
        "- But we can be confident we are putting the effort towards relevant, representative questions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "l_fF_K2Tebib",
      "metadata": {
        "id": "l_fF_K2Tebib"
      },
      "outputs": [],
      "source": [
        "unrolled_gemini_df.to_csv(\"representative_eval_questions.csv\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "UDNXBCiOcLbK",
        "Gw_Og38p4uZE"
      ],
      "name": "curate_new_evals_share.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "curate-evals",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
